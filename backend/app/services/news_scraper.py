"""
ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤
ë§¤ ì‹œê°„ë§ˆë‹¤ ìë™ìœ¼ë¡œ Yahoo Finance, Google News, SEC ê³µì‹œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜´
"""
import yfinance as yf
import feedparser
from datetime import datetime, timezone
from app.database import db
import logging
import time
from newspaper import Article
import asyncio
from urllib.parse import quote

logger = logging.getLogger(__name__)


def parse_rss_date(date_str):
    """RSS ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±"""
    try:
        # RSS ë‚ ì§œ í˜•ì‹ (ì˜ˆ: Wed, 16 Oct 2024 14:30:00 GMT)
        time_struct = time.strptime(date_str, '%a, %d %b %Y %H:%M:%S %Z')
        return datetime(*time_struct[:6], tzinfo=timezone.utc).isoformat()
    except:
        try:
            # ISO í˜•ì‹
            return datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ (UTC)
            return datetime.now(timezone.utc).isoformat()


def extract_full_article(url: str) -> dict:
    """
    URLì—ì„œ ì „ë¬¸(full article)ì„ ì¶”ì¶œ
    Returns: {"content": str, "summary": str, "success": bool}
    """
    try:
        article = Article(url)
        article.download()
        article.parse()

        # ì „ë¬¸ ì¶”ì¶œ
        full_text = article.text

        # ìš”ì•½ë³¸ ìƒì„± (ì²˜ìŒ 500ì)
        summary = full_text[:500] if len(full_text) > 500 else full_text

        if full_text and len(full_text) > 100:  # ìµœì†Œ 100ì ì´ìƒì´ì–´ì•¼ ìœ íš¨í•œ ê¸°ì‚¬ë¡œ ê°„ì£¼
            logger.info(f"âœ… Successfully extracted article ({len(full_text)} characters)")
            return {
                "content": full_text,
                "summary": summary,
                "success": True
            }
        else:
            logger.warning(f"âš ï¸ Article too short or empty ({len(full_text)} characters)")
            return {
                "content": "",
                "summary": "",
                "success": False
            }
    except Exception as e:
        logger.error(f"âŒ Failed to extract article from {url}: {e}")
        return {
            "content": "",
            "summary": "",
            "success": False
        }


async def scrape_yahoo_finance_news(ticker: str, stock_id: str):
    """Yahoo Financeì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì „ë¬¸ ì¶”ì¶œ)"""
    news_added = 0

    try:
        stock = yf.Ticker(ticker)
        news_items = stock.news if hasattr(stock, 'news') else []

        for item in news_items[:5]:  # ìµœê·¼ 5ê°œë§Œ
            try:
                # ìƒˆë¡œìš´ Yahoo Finance API êµ¬ì¡° (content ê°ì²´ ì•ˆì— ë°ì´í„°ê°€ ìˆìŒ)
                content_obj = item.get('content', item)  # í˜¸í™˜ì„±ì„ ìœ„í•´ fallback

                title = content_obj.get('title', '')
                url_obj = content_obj.get('clickThroughUrl') or content_obj.get('canonicalUrl')
                url = url_obj.get('url', '') if url_obj else content_obj.get('link', '')

                if not title or not url:
                    continue

                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                existing = db.client.table("news").select("id").eq("url", url).execute()
                if existing.data:
                    continue

                # ë‚ ì§œ íŒŒì‹± (ìƒˆë¡œìš´ í˜•ì‹: pubDate ë˜ëŠ” displayTime)
                pub_date = content_obj.get('pubDate') or content_obj.get('displayTime')
                if pub_date:
                    published_at = datetime.fromisoformat(pub_date.replace('Z', '+00:00')).isoformat()
                else:
                    published_at = datetime.now(timezone.utc).isoformat()

                # Publisher ì •ë³´ ì¶”ì¶œ
                provider = content_obj.get('provider', {})
                publisher = provider.get('displayName', 'Yahoo Finance') if isinstance(provider, dict) else 'Yahoo Finance'

                # ì „ë¬¸ ì¶”ì¶œ
                logger.info(f"ğŸ“° Extracting full article from {url[:80]}...")
                article_data = extract_full_article(url)

                # ì „ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í•˜ë©´ ìš”ì•½ë³¸ ì‚¬ìš©
                if article_data["success"]:
                    content = article_data["content"]
                    summary = article_data["summary"]
                else:
                    # Fallback: Yahoo Finance APIì˜ summary ì‚¬ìš© (ìƒˆë¡œìš´ êµ¬ì¡°)
                    fallback_summary = content_obj.get('summary', content_obj.get('description', title))
                    content = fallback_summary
                    summary = fallback_summary[:500] if len(fallback_summary) > 500 else fallback_summary
                    logger.warning(f"âš ï¸ Using fallback summary for {url[:80]}")

                # ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°
                impact_score = calculate_impact_score(title, content)

                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                news_data = {
                    "stock_id": stock_id,
                    "title": title,
                    "content": content,
                    "summary": summary,
                    "url": url,
                    "source": publisher,
                    "published_at": published_at,
                    "impact_score": impact_score,
                    "created_at": datetime.now(timezone.utc).isoformat()
                }

                result = db.client.table("news").insert(news_data).execute()
                if result.data:
                    logger.info(f"âœ… Added Yahoo Finance news for {ticker}: {title[:50]}")
                    news_added += 1

            except Exception as e:
                logger.error(f"Error processing Yahoo Finance news item for {ticker}: {e}")
                continue

    except Exception as e:
        logger.error(f"Error fetching Yahoo Finance news for {ticker}: {e}")

    return news_added


async def scrape_google_news_rss(ticker: str, company_name: str, stock_id: str):
    """Google News RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì „ë¬¸ ì¶”ì¶œ)"""
    news_added = 0

    try:
        # Google News RSS URL
        search_query = f"{ticker} OR {company_name} stock"
        encoded_query = quote(search_query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"

        feed = feedparser.parse(rss_url)

        for entry in feed.entries[:5]:  # ìµœê·¼ 5ê°œë§Œ
            try:
                title = entry.get('title', '')
                link = entry.get('link', '')

                if not title or not link:
                    continue

                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                existing = db.client.table("news").select("id").eq("url", link).execute()
                if existing.data:
                    continue

                # ë‚ ì§œ íŒŒì‹±
                published = entry.get('published', '')
                published_at = parse_rss_date(published) if published else datetime.now(timezone.utc).isoformat()

                source = entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else 'Google News'

                # ì „ë¬¸ ì¶”ì¶œ
                logger.info(f"ğŸ“° Extracting full article from {link[:80]}...")
                article_data = extract_full_article(link)

                # ì „ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í•˜ë©´ ìš”ì•½ë³¸ ì‚¬ìš©
                if article_data["success"]:
                    content = article_data["content"]
                    summary = article_data["summary"]
                else:
                    # Fallback: RSSì˜ summary ì‚¬ìš©
                    fallback_summary = entry.get('summary', title)
                    content = fallback_summary
                    summary = fallback_summary[:500] if len(fallback_summary) > 500 else fallback_summary
                    logger.warning(f"âš ï¸ Using fallback summary for {link[:80]}")

                # ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°
                impact_score = calculate_impact_score(title, content)

                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                news_data = {
                    "stock_id": stock_id,
                    "title": title,
                    "content": content,
                    "summary": summary,
                    "url": link,
                    "source": source,
                    "published_at": published_at,
                    "impact_score": impact_score,
                    "created_at": datetime.now(timezone.utc).isoformat()
                }

                result = db.client.table("news").insert(news_data).execute()
                if result.data:
                    logger.info(f"âœ… Added Google News for {ticker}: {title[:50]}")
                    news_added += 1

            except Exception as e:
                logger.error(f"Error processing Google News item for {ticker}: {e}")
                continue

    except Exception as e:
        logger.error(f"Error fetching Google News for {ticker}: {e}")

    return news_added


def calculate_impact_score(title: str, content: str) -> int:
    """
    ë‰´ìŠ¤ì˜ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° (1-5)
    ë†’ì€ ì ìˆ˜ = ì‹œì¥ ì „ì²´ì— ì˜í–¥ì„ ì£¼ëŠ” ì¤‘ìš” ë‰´ìŠ¤
    """
    title_lower = title.lower()
    content_lower = content.lower()

    # ë†’ì€ ì˜í–¥ë„ í‚¤ì›Œë“œ (ì ìˆ˜ 5)
    high_impact_keywords = [
        'federal reserve', 'fed', 'interest rate', 'inflation', 'recession',
        'sec', 'regulation', 'policy', 'earnings report', 'market crash',
        'economic', 'gdp', 'unemployment', 'forex', 'exchange rate',
        'trade war', 'tariff', 'geopolitical', 'bank crisis'
    ]

    # ì¤‘ê°„ ì˜í–¥ë„ í‚¤ì›Œë“œ (ì ìˆ˜ 4)
    medium_impact_keywords = [
        'analyst', 'upgrade', 'downgrade', 'price target', 'market outlook',
        'sector', 'industry', 'merger', 'acquisition', 'ipo'
    ]

    # ì œëª©ì— ë†’ì€ ì˜í–¥ë„ í‚¤ì›Œë“œ í¬í•¨
    for keyword in high_impact_keywords:
        if keyword in title_lower or keyword in content_lower[:500]:
            return 5

    # ì œëª©ì— ì¤‘ê°„ ì˜í–¥ë„ í‚¤ì›Œë“œ í¬í•¨
    for keyword in medium_impact_keywords:
        if keyword in title_lower:
            return 4

    # ê¸°ë³¸ ì ìˆ˜
    return 3


async def scrape_hot_market_news():
    """
    í•«í•œ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘
    ì£¼ìš” ì§€ìˆ˜(S&P 500, Nasdaq, Dow Jones) ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬
    ì‹œì¥ ì „ì²´ì— ì˜í–¥ì„ ì£¼ëŠ” ì¤‘ìš” ë‰´ìŠ¤ë¥¼ ì œê³µ
    """
    news_added = 0

    try:
        # ì²« ë²ˆì§¸ ì¢…ëª©ì„ ê¸°ë³¸ stock_idë¡œ ì‚¬ìš©
        stocks_result = db.client.table("stocks").select("id").limit(1).execute()
        if not stocks_result.data:
            logger.warning("No stocks in database for hot news")
            return 0

        default_stock_id = stocks_result.data[0]['id']

        # ì£¼ìš” ì‹œì¥ ì§€ìˆ˜ ì‹¬ë³¼
        market_indices = ['^GSPC', '^IXIC', '^DJI']  # S&P 500, Nasdaq, Dow Jones

        for index_symbol in market_indices:
            try:
                logger.info(f"ğŸ“° Fetching market news from {index_symbol}...")
                stock = yf.Ticker(index_symbol)
                news_items = stock.news if hasattr(stock, 'news') else []

                for item in news_items[:3]:  # ê° ì§€ìˆ˜ë‹¹ 3ê°œ
                    try:
                        # Yahoo Finance API êµ¬ì¡°
                        content_obj = item.get('content', item)

                        title = content_obj.get('title', '')
                        url_obj = content_obj.get('clickThroughUrl') or content_obj.get('canonicalUrl')
                        url = url_obj.get('url', '') if url_obj else content_obj.get('link', '')

                        if not title or not url:
                            continue

                        # ì¤‘ë³µ í™•ì¸
                        existing = db.client.table("news").select("id").eq("url", url).execute()
                        if existing.data:
                            continue

                        # ë‚ ì§œ íŒŒì‹±
                        pub_date = content_obj.get('pubDate') or content_obj.get('displayTime')
                        if pub_date:
                            published_at = datetime.fromisoformat(pub_date.replace('Z', '+00:00')).isoformat()
                        else:
                            published_at = datetime.now(timezone.utc).isoformat()

                        # Publisher ì •ë³´
                        provider = content_obj.get('provider', {})
                        publisher = provider.get('displayName', 'Market News') if isinstance(provider, dict) else 'Market News'

                        # ì „ë¬¸ ì¶”ì¶œ
                        logger.info(f"ğŸ“° Extracting market news from {url[:80]}...")
                        article_data = extract_full_article(url)

                        if article_data["success"]:
                            content = article_data["content"]
                            summary = article_data["summary"]
                        else:
                            fallback_summary = content_obj.get('summary', content_obj.get('description', title))
                            content = fallback_summary
                            summary = fallback_summary[:500] if len(fallback_summary) > 500 else fallback_summary

                        # ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°
                        impact_score = calculate_impact_score(title, content)

                        news_data = {
                            "stock_id": default_stock_id,
                            "title": title,
                            "content": content,
                            "summary": summary[:500],
                            "url": url,
                            "source": publisher,
                            "published_at": published_at,
                            "impact_score": impact_score,
                            "created_at": datetime.now(timezone.utc).isoformat()
                        }

                        result = db.client.table("news").insert(news_data).execute()
                        if result.data:
                            logger.info(f"âœ… Added market news (impact: {impact_score}): {title[:50]}")
                            news_added += 1

                    except Exception as e:
                        logger.error(f"Error processing market news from {index_symbol}: {e}")
                        continue

                await asyncio.sleep(1)  # Rate limiting

            except Exception as e:
                logger.error(f"Error fetching news from {index_symbol}: {e}")

        logger.info(f"ğŸ”¥ Added {news_added} market news articles")

    except Exception as e:
        logger.error(f"Error in scrape_hot_market_news: {e}")

    return news_added


async def fetch_all_news():
    """ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ”„ Starting news scraping job...")

    try:
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  ì¢…ëª© ê°€ì ¸ì˜¤ê¸°
        stocks_result = db.client.table("stocks").select("*").execute()
        stocks = stocks_result.data if stocks_result.data else []

        if not stocks:
            logger.warning("No stocks found in database")
            return

        total_news_added = 0

        # í•«í•œ ì‹œì¥ ë‰´ìŠ¤ ë¨¼ì € ìˆ˜ì§‘
        logger.info("ğŸ”¥ Fetching hot market news...")
        hot_news_count = await scrape_hot_market_news()
        total_news_added += hot_news_count

        for stock in stocks:
            ticker = stock['ticker']
            stock_id = stock['id']
            company_name = stock.get('company_name', ticker)

            logger.info(f"ğŸ“° Fetching news for {ticker} ({company_name})...")

            # Yahoo Financeì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            yahoo_count = await scrape_yahoo_finance_news(ticker, stock_id)
            total_news_added += yahoo_count

            # Google Newsì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            google_count = await scrape_google_news_rss(ticker, company_name, stock_id)
            total_news_added += google_count

            # Rate limiting ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            await asyncio.sleep(2)

        logger.info(f"âœ¨ News scraping completed! Added {total_news_added} new articles")

    except Exception as e:
        logger.error(f"Error in fetch_all_news: {e}")
